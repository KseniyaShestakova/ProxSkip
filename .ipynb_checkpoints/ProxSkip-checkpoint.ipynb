{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590523ad",
   "metadata": {},
   "source": [
    "# ProxSkip\n",
    "\n",
    "## Постановка задачи\n",
    "\n",
    "Рассмотрим следующую задачу минимизации:\n",
    "$$ \\min_{x \\in \\mathbb{R}^d} (f(x) + \\psi(x))\\ \\ \\ \\ \\ \\ \\ \\ (1)$$\n",
    "где $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ - выпуклая функция, $\\psi: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$ - регуляризатор.\n",
    "\n",
    "### Proximal Gradient Descent\n",
    "Proximal Gradient Descent - подход к решению задачи $(1)$. Это итеративный алгоритм со следующим шагом:\n",
    "$$ x_{t+1} = prox_{\\gamma_t \\psi}(x_t  - \\gamma_t \\nabla f(x_t)) $$\n",
    "Где:\n",
    "1. $x_t$ - приближение ответа в момент времени $t$.\n",
    "2. $\\gamma_t$ - шаг в момент времени $t$.\n",
    "3. $prox_{\\gamma \\psi} := argmin_{y \\in \\mathbb{R}^d} \\left( \\frac12 \\|y-x\\|^2 + \\gamma \\psi(y) \\right)$ - оператор приближения.\n",
    "\n",
    "Как правило, вычисление градиента в Proximal Gradient Descent является более вычислительно сложным, чем вычисление оператора приближения. Однако в данной статье рассматривается ситуация, когда оператор приближения по сложности вычисления сравним с градиентом.\n",
    "\n",
    "### Распределенные вычисления\n",
    "Рассмотрим следующую задачу. Пусть есть $n$ кластеров/нод/вычислительных клиентов, и $i$-ый кластер вычисляет функцию $f_i: \\mathbb{R}^d \\rightarrow \\mathbb{R}$. Рассмотрим задачу минимизации функции $f(x) := \\frac1n \\sum\\limits_{i=1}^n f_i(x)$:\n",
    "$$ \\min_{x \\in \\mathbb{R}^d} f(x) $$\n",
    "Эта задача актуальна для современного машинного обучения, т.к. является абстракцией над задачей минимизации эмпирического риска. \\\n",
    "Посмотрим на частный случай задачи $(1)$ в распределенном виде:\n",
    "$$ \\min_{x_1, \\dots, x_n \\in \\mathbb{R}^d} \\frac1n \\sum\\limits_{i=1}^{n} f_i(x_i) + \\psi(x_1, \\dots, x_n)$$\n",
    "где $\\psi(x_1, \\dots, x_n) = 0$ при $x_1 = \\dots = x_n$, $+\\infty$ иначе. \\\n",
    "При такой задаче локальный подсчет функции $f_i$ на $i$-ом кластере - не очень сложная вычислительная задача, а главная трудность кроется в коммуникации между кластерами. \\\n",
    "Подобные задачи возникают в федеративном обучении. Разрабатываются алгоритмы для сокращения коммуникации и достижения хорошего временного соотношения для коммуникации и вычислений. Бегло посмотрим на вклад данной статьи в решении подобных задач.\n",
    "\n",
    "### Идеи и обощения ProxSkip\n",
    "ProxSkip - обобщение метода Proximal Gradient Descent для решения задачи $(1)$. Суть метода заключается в том, что вместо вычисления значения оператора $prox$ на каждом шаге, он вычисляется с некоторой вероятностью $p \\in (0, 1]$. \\\n",
    "Для того, чтобы эффективность метода была доказуема, используется валидационное слагаемое $h_t$. \\\n",
    "В статье описывается Scaffnew - метод применения ProxSkip к задачам федеративного обучения. \n",
    "\n",
    "Так же рассматриваются следующие **расширения** этого метода:\n",
    "1. Переход от детерминированного вычисления градиента к стохастическому.\n",
    "2. Переход от вычислений с центральным сервером к децентрализованным вычислениям."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f9d7d",
   "metadata": {},
   "source": [
    "## Реализация в коде и эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9c0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b830f52",
   "metadata": {},
   "source": [
    "Начнем с реализации алгоритма ProxSkip в общем виде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c4c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProxSkip(gamma, p, x0, h0, T, f, nabla_f, phi):\n",
    "    '''\n",
    "    gamma - stepsize\n",
    "    p - probability of skipping the prox\n",
    "    x0 - initial iterate\n",
    "    h0 - initial control variate\n",
    "    T - number of iterations\n",
    "    f - smooth function\n",
    "    nabla_f - grad of smooth function\n",
    "    phi - proper, closed and convex reqularizer\n",
    "    '''\n",
    "    curr_x = x0\n",
    "    curr_h = h0\n",
    "    \n",
    "    coin = sps.bernoulli(p)\n",
    "    \n",
    "    for t in range(T):\n",
    "        hat_x = curr_x - gamma * (nabla_f(curr_x) - curr_h)\n",
    "        \n",
    "        calc_prox = coin.rvs(size=1)[0]\n",
    "        \n",
    "        if calc_prox:\n",
    "            prox_func = lambda x : ((gamma * phi(x)) / p)\n",
    "            curr_x = prox(prox_func, hat_x - (gamma * curr_h) / p)\n",
    "        else:\n",
    "            curr_x = hat_x\n",
    "            \n",
    "        curr_h += p * (curr_x - hat_x) / gamma\n",
    "        \n",
    "    return curr_x   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6c1f9",
   "metadata": {},
   "source": [
    "Перейдем к реализации Scaffnew - применения ProxSkip-а к задачам федеративного обучения. Здесь предполагается, что у нас есть несколько параллельно работающих кластеров, и функция $g$ принимает массив $x$ и возвращает массив, $i$-ая компонента которого - $g_i(x_i)$, причем компоненты вычисляются параллельно и независимо друг от друга.\n",
    "\n",
    "В методе используется функция update_x, которая должна параллельно для $i=1\\dots n$ делать следующее: если $calc\\_prox[i] = 1$, то заменить $curr\\_x[i]$ на $\\frac1n \\sum\\limits_{i=1}^{n} \\hat x [i]$, иначе заменить $curr\\_x[i]$ на $\\hat x [i]$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b818029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaffnew(gamma, p, x0, h0, T, g, update_x):\n",
    "    '''\n",
    "    gamma - stepsize\n",
    "    p - probability of skipping the prox\n",
    "    x0 - initial iterate, an array with equal components\n",
    "    h0 - initial control variate, an array with zero sum of elements\n",
    "    T - number of iterations\n",
    "    g - a function to be computed in parallel\n",
    "    update_x - parralel calculation of curr_x\n",
    "    '''\n",
    "    curr_x = x0\n",
    "    curr_h = h0\n",
    "    n = len(x0)\n",
    "    \n",
    "    coin = sps.bernoulli(p)\n",
    "    \n",
    "    for t in range(T):\n",
    "        calc_prox = coin.rvs(size=n)\n",
    "        \n",
    "        hat_x = curr_x - gamma * (g(curr_x) - curr_h)\n",
    "        \n",
    "        update_x(curr_x, hat_x, calc_prox)\n",
    "        \n",
    "        curr_h += p * (curr_x - hat_x) / gamma\n",
    "        \n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a9497",
   "metadata": {},
   "source": [
    "Перейдем к реализации Decentralized Scaffnew - обобщения Scaffnew, где вместо обычного среднего вычисляется взвешенное среднее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf1415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecentralizedScaffnew(gamma, tau, p, x0, h0, W, T, f, nabla_f):\n",
    "    '''\n",
    "    gamma, tau - stepsizes\n",
    "    p - probability of scipping the prox\n",
    "    x0 - initial iterate, an array with equal components\n",
    "    h0 - initial control variate, an array with zero elements\n",
    "    W - weights for averaging\n",
    "    f - smooth function\n",
    "    grad_f - gradient of f\n",
    "    '''\n",
    "    curr_x = x0\n",
    "    curr_h = h0\n",
    "    n = len(x0)\n",
    "    \n",
    "    coin = sps.bernoulli(p)\n",
    "    \n",
    "    for t in range(T):\n",
    "        hat_x = curr_x - gamma * (nabla_f(curr_x) - curr_h)\n",
    "        \n",
    "        calc_prox = coin.rvs(size=n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            if calc_prox[i]:\n",
    "                k = (gamma * tau) / p\n",
    "                curr_x[i] = (1 - k) * hat_x[i] + k * np.dot(W[i], x.T)\n",
    "                curr_h[i] += p * (curr_x[i] - hat_x[i]) / gamma\n",
    "            else:\n",
    "                curr_x[i] = hat_x[i]\n",
    "                # curr_h[i] = curr_h[i] - remains the same\n",
    "            \n",
    "        return curr_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b8b53",
   "metadata": {},
   "source": [
    "## Теоретическая часть\n",
    "**Предположение 3.1.** Пусть $f$ - $L$-гладкая и $\\mu$-сильно выпуклая функция. \\\n",
    "**Предположение 3.2.** Пусть $\\psi$ - замкнутая, выпуклая, проксимально дружественная функция. \\\n",
    "В таких предположениях задача $(1)$ имеет единственное решение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dffd6d",
   "metadata": {},
   "source": [
    "### Вспомогательные леммы\n",
    "Рассмотрим леммы, которые используются в статье для доказательства основной теоремы.\n",
    "\n",
    "**Лемма 3.3 (firm nonexpansiveness).** Пусть выполнено предположение 3.2. Пусть $P(x) := \\text{prox}_{\\frac{\\gamma}{p} \\psi}(x),\\ Q(x) := x - P(x)$. Тогда \n",
    "$$ \\| P(x) - P(y) \\|^2 + \\| Q(x) - Q(y) \\|^2 \\le \\| x - y \\|^2 \\ \\ \\ \\ \\ \\ \\ (10) $$\n",
    "для всех $x, y \\in \\mathbb{R}^d$ и любых $\\gamma,\\ p > 0$. \\\n",
    "**Доказательство:** \\\n",
    "Перепишем неравенство $(10)$ в эквивалентном виде:\n",
    "$$ \\| P(x) - P(y) \\|^2 + \\| x - y \\|^2 + \\| P(x) - P(y) \\|^2 - \n",
    "2 \\langle x - y, P(x) - P(y) \\rangle \\le \\| x - y \\|^2 $$\n",
    "что равносильно:\n",
    "$$ \\langle x - y, P(x) - P(y) \\rangle \\ge \\| P(x) - P(y) \\|^2 $$\n",
    "Это неравенство было доказано на лекции 11. \n",
    "\n",
    "Положим $x_*$ - решение задачи $(1)$, $h_* := \\nabla f(x_*)$ (нашей целью будет показать, что $h_t$ действительно сходятся к $\\nabla f(x_*)$. Для этого рассмотрим функцию Ляпунова:\n",
    "$$ \\Psi_t := \\| x_t - x_* \\|^2 + \\frac{\\gamma^2}{p^2} \\| h_t - h_* \\|^2 $$\n",
    "Для удобства так же определим $w_t := x_t - \\gamma \\nabla f(x_t),\\ w_* := x_* - \\gamma \\nabla f(x_*)$\n",
    "\n",
    "**Лемма 3.4.** В предположениях 3.1 и 3.2, $\\gamma > 0,\\ 0 < p \\le 1$ верно: \n",
    "$$ \\mathbb{E}[\\Psi_{t+1}] \\le \\| w_t - w_* \\|^2 + (1 - p^2) \\frac{\\gamma^2}{p^2} \\| h_t - h_* \\|^2, $$\n",
    "где математическое ожидание берется по $\\theta_t$ - случайной величине, говорящей, считается ли на шаге $t$ значение проксимального оператора. $\\theta_t \\sim Bern(p)$ - распределение Бернулли с параметром $p$.\n",
    "\n",
    "**Лемма 3.5.** Пусть предположение 3.1 выполнено при некотором $\\mu > 0$. Возьмем $0 < \\gamma \\le \\frac1L$. Тогда:\n",
    "$$ \\| w_t - w_* \\|^2 \\le (1 - \\gamma \\mu) \\| x_t - x_* \\|^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f2c75",
   "metadata": {},
   "source": [
    "### О сходимости ProxSkip\n",
    "**Теорема 3.6.** Пусть выполнены предположения 3.1 и 3.2, и $0 \\gamma \\le \\frac1L,\\ 0 < p \\le 1.$ Тогда \n",
    "$$ \\mathbb{E}[\\Psi_T] \\le (1 - \\zeta)^T \\Psi_0,\\ \\ \\ \\ \\ \\ \\ (15) $$\n",
    "где $\\zeta := \\min \\{ \\gamma \\mu, p^2 \\}$.\n",
    "\n",
    "**Доказательство:** \\\n",
    "Комбинируя леммы 3.4 и 3.5, получаем:\n",
    "$$ \\mathbb{E}[\\Psi_{t+1}] \\le \n",
    " \\| w_t - w_* \\|^2 + (1 - p^2) \\frac{\\gamma^2}{p^2} \\| h_t - h_* \\|^2 \\le \\newline \\le\n",
    "  (1 - \\gamma \\mu) \\| x_t - x_* \\|^2  + (1 - p^2) \\frac{\\gamma^2}{p^2} \\| h_t - h_* \\|^2 \\le \\newline \\le\n",
    "  (1 - \\zeta) ( \\| x_t - x_* \\|^2 + \\frac{\\gamma^2}{p^2} \\| h_t - h_* \\|^2 ) = (1 - \\zeta) \\Psi_t$$\n",
    "Тогда по индукции несложно доказать, что $ \\mathbb{E}[\\Psi_T] \\le (1 - \\zeta)^T \\Psi_0 $.\n",
    "\n",
    "**О подборе шага и $p$.** \\\n",
    "Во-первых, при $p=1$ ProxSkip эквивалентен ProxGD и имеет такую же сложность. \\\n",
    "Во-вторых, если мы зафиксируем размер шага $\\gamma > 0$, то для всех $p \\in [\\sqrt{\\gamma \\mu}, 1]$ сложность будет одинаковой, т.к. сложность определяется значением $\\zeta = \\min\\{\\gamma \\mu, p^2 \\}$. Посмотрев на $(15)$, можно понять, что при $T \\ge \\frac{1}{\\zeta}\\log{\\frac{1}{\\varepsilon}}$ верно: $\\mathbb{E}[\\Psi_T] \\le \\varepsilon \\Psi_0$. Т.е. ожидаемое количество вычислений проксимального оператора: \\\n",
    "$$ pT \\sim \\max\\{\\frac{p}{\\gamma \\mu}, \\frac1p\\} \\log{\\frac{1}{\\varepsilon}}$$\n",
    "Из замечания выше следует, что оптимальная вероятность $p = \\sqrt{\\mu \\gamma}$,  и для максимально быстрой сходимости надо выбирать самый большой допустимый теоремой 3.6. шаг: $\\gamma = \\frac{1}{L}$. Выпишем, как надо подбирать параметры для ProxSkip, в отдельное следствие. Введем $\\kappa = \\frac{L}{\\mu}$.\n",
    "\n",
    "**Следствие 3.7.** При $\\gamma = \\frac1L,\\ p = \\frac{1}{\\kappa}$ ожидаемая итерационная сложность алгоритма ProxSkip - $O(\\kappa \\log\\frac{1{\\varepsilon}})$, ожидаемая оракульная сложность - $O(\\sqrt{\\kappa} \\log\\frac1{\\varepsilon})$ (оракул - подсчет проксимального оператора)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a3f42",
   "metadata": {},
   "source": [
    "### Scaffnew\n",
    "Теперь, как это ранее обещалось, применим ProxSkip для задачи федеративного обучения.\n",
    "\n",
    "**Постановка задачи.** Требуется минимизировать среднее $n$ функций, подсчитываемых на $n$ устройствах:\n",
    "$$ \\min\\limits_{x \\in \\mathbb{R}^d}\\left\\{ f(x) := \\frac1n \\sum\\limits_{i=1}^n f_i(x) \\right\\}  $$\n",
    "\n",
    "**Алгоритм Scaffnew.** \\\n",
    "**Шаг 1.** На каждом устройстве по отдельности обновить текущее значение локальной компоненты $x$: для $i \\in [n]$ изменить $x_{i,t}$. \\\n",
    "**Шаг 2.** На каждом устройстве по отдельности обновить текущее значение локальной контрольной переменной $h$: для $i \\in [n]$ изменить $h_{i,t}$. \\\n",
    "**Шаг 3.** С вероятностью $p$ для каждой итерации усреднить значения  $x$, полученные на клиентах.\n",
    "\n",
    "**Предположение 4.1.** Пусть все $f_i$ - $L$-гладкие и $\\mu$-сильновыпуклые.\n",
    "\n",
    "**Следствие 4.7 (о сходимости Scaffnew, из Т.3.6.).** Пусть выполнено предположение 4.1 и $\\gamma = \\frac{1}{L},\\ p = \\frac1{\\sqrt{\\kappa}},\\ g_{i,t}(x_{i,t}) = \\nabla f_i(x_{i,t})$. Тогда итерационная сложность Scaffnew - $O(\\kappa \\log\\frac{1}{\\varepsilon})$, оракульная сложность - $O(\\sqrt{\\kappa}\\log\\frac1{\\varepsilon})$ (здесь оракул - усреднение значений на кластерах)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774aa12",
   "metadata": {},
   "source": [
    "### Стохастический градиент\n",
    "Как мы знаем, часто подсчет полного градиента - это вычислительно сложная задача, поэтому на практике часто используется подсчет стохастического градиента. Для анализа мы будем использовать только несмещенные оценки градиента.\n",
    "\n",
    "**Предположение 5.1 (несмещенность).** Для всех $t \\ge 0$ верно:\n",
    "$$ \\mathbb{E}[g_t(x_t) | x_t] = \\nabla f(x_t) $$\n",
    "**Предположение 5.2 (ожидаемая гладкость).** Существуют константы $ A \\ge 0$ и $C \\ge 0$ такие что для всех $t \\ge 0$ верно:\n",
    "$$ \\mathbb{E} [\\|  g_t(x_t) - \\nabla f(x_*) \\|^2 | x_t] \\le 2 A D_f(x_t, x_*) + C $$\n",
    "**Предположение 5.3 (ограниченная вариация).** Для всех $t \\ge 0$ верно:\n",
    "$$ \\mathbb{Var}[g_t(x_t) | x_t] \\le \\sigma^2 $$\n",
    "**Лемма 5.4.** Пусть выполнены предположения 5.1 и 5.3, а также $f$ - выпуклая и $L$-гладкая функция, тогда предположение 5.2 верно при $A=L, C=\\sigma^2$. \n",
    "\n",
    "**Теорема 5.5.** Пусть выполнены предположения 3.1, 3.2, 5.2 и 5.1. Пусть $0 < \\gamma < \\frac1A$ и $0 < p \\le 1$. Тогда есть следующая оценка сходимости стохастического ProxSkip:\n",
    "$$ \\mathbb{E}[\\Psi_t] \\le (1 - \\zeta)^T \\Psi_0 + \\frac{\\gamma^2 C}{\\zeta}$$\n",
    "Внимательные слушатели курса методов оптимизации в МФТИ могут заметить, что так же как в случае с переходом от GD к стохастическому GD, при переходе от ProxSkip к стохастическому GD в оценке сходимости появилась добавка - в нашем случае - $\\frac{\\gamma^2 C}{\\zeta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c27242e",
   "metadata": {},
   "source": [
    "### Децентрализованное обучение\n",
    "Пусть теперь взаимодействие - это не просто усреднение по всем нодам, а более сложный процесс. При каждом взаимодействии в ноде $i$ оказывается взвешенное среднее остальных нод с весами $W_{i1}, \\dots, W_{in}$ соответственно. При этом ноды $i$ и $j$ обмениваются данными только если $W_{ij} \\ne 0,\\ W_{ji} \\ne 0$. \\\n",
    "**Матрица весов.** $W$ - *mixing matrix*. Она должна быть симметричной, положительно полуопределенной, сумма чисел в каждой ее строке и каждом столбце должна быть равна 1. \\\n",
    "Тогда исходную задачу можно переформулировать так:\n",
    "$$ \\min\\limits_{x \\in \\mathbb{R}^{dn}} f(x)\\ s.t.\\ (I - W)x = 0 $$ \n",
    "Здесь $I$ - единичная матрица подходящего размера. \\\n",
    "Обозначим в качестве $\\textbf{L}$ квадратный корень из $I-W$ и определим индикаторную функцию $\\psi$ следующим образом: $\\psi(0) = 0,\\ \\forall y \\ne 0\\ \\psi(y) = +\\infty$. Ограничение $(I - W)x = 0$ эквивалентно ограничению $\\textbf{L}x = 0$, а значит, нашу задачу можно переписать в виде:\n",
    "$$ \\min\\limits_{x \\in \\mathbb{R}^{dn}} f(x) + \\psi(\\textbf{L}x) $$\n",
    "**Определение.** Спектральным зазором матрицы $W$ назовем разность между двумя самыми большими собственными значениями матрицы $W$. Введем обозначение для спектрального зазора в нащем случае: $\\delta := 1 - \\lambda_2(W)$. \\\n",
    "**Теорема 5.7.** Пусть $f$ удовлетворяет предположению 4.1. Пусть $0 < p \\le 1, \\gamma < \\frac1L, \\tau \\le \\frac{p}{\\gamma}$, тогда среднее по $x$ на итерации $T$ - $\\overline{x_T}$ - удовлетворяет соотношению:\n",
    "$$ \\mathbb{E}[ \\| \\overline{x_T} - x_* \\|^2 ] \\le (1 - \\min(\\gamma \\mu, p \\gamma \\tau \\delta))^T \\Phi_0,$$\n",
    "где $\\Phi_0 \\le \\| x_0 - x_* \\|^2 + \\frac{\\gamma}{p \\tau \\delta n} \\sum\\limits_{i=1}^{n} \\| \\nabla f_i(x_*) \\|^2 $. \\\n",
    "Если подставить $\\tau = \\frac{p}{\\gamma}$, то получим оценку на итерационную сходимость $O(\\kappa + \\frac{1}{p^2 \\delta})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882741ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
