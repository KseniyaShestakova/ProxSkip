{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590523ad",
   "metadata": {},
   "source": [
    "# ProxSkip\n",
    "\n",
    "## Постановка задачи\n",
    "\n",
    "Рассмотрим следующую задачу минимизации:\n",
    "$$ \\min_{x \\in \\mathbb{R}^d} (f(x) + \\psi(x))\\ \\ \\ \\ \\ \\ \\ \\ (1)$$\n",
    "где $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ - выпуклая функция, $\\psi: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$ - регуляризатор.\n",
    "\n",
    "### Proximal Gradient Descent\n",
    "Proximal Gradient Descent - подход к решению задачи $(1)$. Это итеративный алгоритм со следующим шагом:\n",
    "$$ x_{t+1} = prox_{\\gamma_t \\psi}(x_t  - \\gamma_t \\nabla f(x_t)) $$\n",
    "Где:\n",
    "1. $x_t$ - приближение ответа в момент времени $t$.\n",
    "2. $\\gamma_t$ - шаг в момент времени $t$.\n",
    "3. $prox_{\\gamma \\psi} := argmin_{y \\in \\mathbb{R}^d} \\left( \\frac12 \\|y-x\\|^2 + \\gamma \\psi(y) \\right)$ - оператор приближения.\n",
    "\n",
    "Как правило, вычисление градиента в Proximal Gradient Descent является более вычислительно сложным, чем вычисление оператора приближения. Однако в данной статье рассматривается ситуация, когда оператор приближения по сложности вычисления сравним с градиентом.\n",
    "\n",
    "### Распределенные вычисления\n",
    "Рассмотрим следующую задачу. Пусть есть $n$ кластеров/нод/вычислительных клиентов, и $i$-ый кластер вычисляет функцию $f_i: \\mathbb{R}^d \\rightarrow \\mathbb{R}$. Рассмотрим задачу минимизации функции $f(x) := \\frac1n \\sum\\limits_{i=1}^n f_i(x)$:\n",
    "$$ \\min_{x \\in \\mathbb{R}^d} f(x) $$\n",
    "Эта задача актуальна для современного машинного обучения, т.к. является абстракцией над задачей минимизации эмпирического риска. \\\n",
    "Посмотрим на частный случай задачи $(1)$ в распределенном виде:\n",
    "$$ \\min_{x_1, \\dots, x_n \\in \\mathbb{R}^d} \\frac1n \\sum\\limits_{i=1}^{n} f_i(x_i) + \\psi(x_1, \\dots, x_n)$$\n",
    "где $\\psi(x_1, \\dots, x_n) = 0$ при $x_1 = \\dots = x_n$, $+\\infty$ иначе. \\\n",
    "При такой задаче локальный подсчет функции $f_i$ на $i$-ом кластере - не очень сложная вычислительная задача, а главная трудность кроется в коммуникации между кластерами. \\\n",
    "Подобные задачи возникают в федеративном обучении. Разрабатываются алгоритмы для сокращения коммуникации и достижения хорошего временного соотношения для коммуникации и вычислений. Бегло посмотрим на вклад данной статьи в решении подобных задач.\n",
    "\n",
    "### Идеи и обощения ProxSkip\n",
    "ProxSkip - обобщение метода Proximal Gradient Descent для решения задачи $(1)$. Суть метода заключается в том, что вместо вычисления значения оператора $prox$ на каждом шаге, он вычисляется с некоторой вероятностью $p \\in (0, 1]$. \\\n",
    "Для того, чтобы эффективность метода была доказуема, используется валидационное слагаемое $h_t$. \\\n",
    "В статье описывается Scaffnew - метод применения ProxSkip к задачам федеративного обучения. \n",
    "\n",
    "Так же рассматриваются следующие **расширения** этого метода:\n",
    "1. Переход от детерминированного вычисления градиента к стохастическому.\n",
    "2. Переход от вычислений с центральным сервером к децентрализованным вычислениям."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f9d7d",
   "metadata": {},
   "source": [
    "## Реализация в коде и эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9c0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b830f52",
   "metadata": {},
   "source": [
    "Начнем с реализации алгоритма ProxSkip в общем виде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c4c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProxSkip(gamma, p, x0, h0, T, f, nabla_f, phi):\n",
    "    '''\n",
    "    gamma - stepsize\n",
    "    p - probability of skipping the prox\n",
    "    x0 - initial iterate\n",
    "    h0 - initial control variate\n",
    "    T - number of iterations\n",
    "    f - smooth function\n",
    "    nabla_f - grad of smooth function\n",
    "    phi - proper, closed and convex reqularizer\n",
    "    '''\n",
    "    curr_x = x0\n",
    "    curr_h = h0\n",
    "    \n",
    "    coin = sps.bernoulli(p)\n",
    "    \n",
    "    for t in range(T):\n",
    "        hat_x = curr_x - gamma * (nabla_f(curr_x) - curr_h)\n",
    "        \n",
    "        calc_prox = coin.rvs(size=1)[0]\n",
    "        \n",
    "        if calc_prox:\n",
    "            prox_func = lambda x : ((gamma * phi(x)) / p)\n",
    "            curr_x = prox(prox_func, hat_x - (gamma * curr_h) / p)\n",
    "        else:\n",
    "            curr_x = hat_x\n",
    "            \n",
    "        curr_h += p * (curr_x - hat_x) / gamma\n",
    "        \n",
    "    return curr_x   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6c1f9",
   "metadata": {},
   "source": [
    "Перейдем к реализации Scaffnew - применения ProxSkip-а к задачам федеративного обучения. Здесь предполагается, что у нас есть несколько параллельно работающих кластеров, и функция $g$ принимает массив $x$ и возвращает массив, $i$-ая компонента которого - $g_i(x_i)$, причем компоненты вычисляются параллельно и независимо друг от друга.\n",
    "\n",
    "В методе используется функция update_x, которая должна параллельно для $i=1\\dots n$ делать следующее: если $calc\\_prox[i] = 1$, то заменить $curr\\_x[i]$ на $\\frac1n \\sum\\limits_{i=1}^{n} \\hat x [i]$, иначе заменить $curr\\_x[i]$ на $\\hat x [i]$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b818029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaffnew(gamma, p, x0, h0, T, g, update_x):\n",
    "    '''\n",
    "    gamma - stepsize\n",
    "    p - probability of skipping the prox\n",
    "    x0 - initial iterate, an array with equal components\n",
    "    h0 - initial control variate, an array with zero sum of elements\n",
    "    T - number of iterations\n",
    "    g - a function to be computed in parallel\n",
    "    update_x - parralel calculation of curr_x\n",
    "    '''\n",
    "    curr_x = x0\n",
    "    curr_h = h0\n",
    "    n = len(x0)\n",
    "    \n",
    "    coin = sps.bernoulli(p)\n",
    "    \n",
    "    for t in range(T):\n",
    "        calc_prox = coin.rvs(size=n)\n",
    "        \n",
    "        hat_x = curr_x - gamma * (g(curr_x) - curr_h)\n",
    "        \n",
    "        update_x(curr_x, hat_x, calc_prox)\n",
    "        \n",
    "        curr_h += p * (curr_x - hat_x) / gamma\n",
    "        \n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a9497",
   "metadata": {},
   "source": [
    "Перейдем к реализации Decentralized Scaffnew - обобщения Scaffnew, где вместо обычного среднего вычисляется взвешенное среднее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf1415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecentralizedScaffnew(gamma, tau, p, x0, h0, W, T, f, nabla_f):\n",
    "    '''\n",
    "    gamma, tau - stepsizes\n",
    "    p - probability of scipping the prox\n",
    "    x0 - initial iterate, an array with equal components\n",
    "    h0 - initial control variate, an array with zero elements\n",
    "    W - weights for averaging\n",
    "    f - smooth function\n",
    "    grad_f - gradient of f\n",
    "    '''\n",
    "    curr_x = x0\n",
    "    curr_h = h0\n",
    "    n = len(x0)\n",
    "    \n",
    "    coin = sps.bernoulli(p)\n",
    "    \n",
    "    for t in range(T):\n",
    "        hat_x = curr_x - gamma * (nabla_f(curr_x) - curr_h)\n",
    "        \n",
    "        calc_prox = coin.rvs(size=n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            if calc_prox[i]:\n",
    "                k = (gamma * tau) / p\n",
    "                curr_x[i] = (1 - k) * hat_x[i] + k * np.dot(W[i], x.T)\n",
    "                curr_h[i] += p * (curr_x[i] - hat_x[i]) / gamma\n",
    "            else:\n",
    "                curr_x[i] = hat_x[i]\n",
    "                # curr_h[i] = curr_h[i] - remains the same\n",
    "            \n",
    "        return curr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882741ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
